# Yner Chat - Assistant IA avec Recherche Web

Une application de chat intelligente propulsÃ©e par Ollama (Mistral) avec capacitÃ© de recherche web en temps rÃ©el via DuckDuckGo.

## ğŸ“‹ Description

Yner Chat est une interface de chat moderne qui combine la puissance d'un modÃ¨le de langage local (Mistral via Ollama) avec des capacitÃ©s de recherche web automatique. L'application effectue automatiquement une recherche web pour chaque requÃªte, permettant d'obtenir des rÃ©ponses enrichies avec des informations rÃ©centes et vÃ©rifiables.

## âœ¨ FonctionnalitÃ©s

- ğŸ¤– **Chat IA Local** : Utilise Mistral via Ollama pour des rÃ©ponses rapides et privÃ©es
- ğŸŒ **Recherche Web Automatique** : Chaque prompt dÃ©clenche une recherche web via DuckDuckGo
- ğŸ“Š **Affichage des Sources** : Les sources web sont affichÃ©es avec les rÃ©ponses
- âš¡ **Streaming en Temps RÃ©el** : Les rÃ©ponses s'affichent progressivement
- ğŸ¨ **Interface Moderne** : UI Ã©lÃ©gante avec animations Framer Motion
- ğŸ³ **DÃ©ploiement Docker** : Configuration complÃ¨te avec Docker Compose
- ğŸ” **Indicateurs Visuels** : Feedback visuel pendant la recherche web

## ğŸ—ï¸ Architecture

Le projet est composÃ© de trois services principaux :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend   â”‚â”€â”€â”€â”€â”€â–¶â”‚  Backend    â”‚â”€â”€â”€â”€â”€â–¶â”‚   Ollama    â”‚
â”‚  (Next.js)  â”‚      â”‚  (Express)  â”‚      â”‚  (Mistral)  â”‚
â”‚   Port 3000 â”‚      â”‚   Port 3001 â”‚      â”‚  Port 11434 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚ DuckDuckGo  â”‚
                     â”‚    API      â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Frontend (Next.js 15)
- React 19 avec TypeScript
- Tailwind CSS pour le styling
- Framer Motion pour les animations
- Interface de chat avec streaming en temps rÃ©el
- Affichage formatÃ© des rÃ©ponses avec sources

### Backend (Node.js/Express)
- API REST pour la gÃ©nÃ©ration de texte
- IntÃ©gration avec Ollama
- Recherche web automatique via DuckDuckGo
- Streaming des rÃ©ponses
- Extraction et formatage des rÃ©sultats web

### Ollama
- ModÃ¨le Mistral pour la gÃ©nÃ©ration de texte
- ExÃ©cution locale du modÃ¨le IA

## ğŸš€ Installation

### PrÃ©requis

- [Docker](https://www.docker.com/get-started) (v20.10+)
- [Docker Compose](https://docs.docker.com/compose/install/) (v2.0+)
- Au moins 8 GB de RAM disponible pour Ollama
- Connexion internet pour la recherche web

### Installation avec Docker (RecommandÃ©)

#### 1. Cloner le projet

```bash
git clone <votre-repo>
cd IA
```

#### 2. Lancer l'application en production

```bash
docker-compose up -d
```

#### 3. TÃ©lÃ©charger le modÃ¨le Mistral

```bash
docker exec -it ia-ollama-1 ollama pull mistral:latest
```

#### 4. AccÃ©der Ã  l'application

- Frontend : [http://localhost:3000](http://localhost:3000)
- Backend API : [http://localhost:3001](http://localhost:3001)
- Ollama : [http://localhost:11434](http://localhost:11434)

### Installation en mode dÃ©veloppement

#### 1. Lancer avec hot-reload

```bash
docker-compose -f docker-compose.dev.yml up
```

Le mode dÃ©veloppement inclut :
- Hot-reload pour le frontend (Next.js Turbopack)
- Nodemon pour le backend
- Volumes montÃ©s pour modification en temps rÃ©el

#### 2. Installer le modÃ¨le

```bash
docker exec -it ia-ollama-1 ollama pull mistral:latest
```

### Installation locale (sans Docker)

#### Backend

```bash
cd backend
npm install
npm run dev
```

#### Frontend

```bash
cd frontend
npm install
npm run dev
```

**Note** : Pour l'installation locale, vous devez installer Ollama sÃ©parÃ©ment depuis [ollama.ai](https://ollama.ai) et tÃ©lÃ©charger le modÃ¨le Mistral.

## âš™ï¸ Configuration

### Variables d'environnement

#### Frontend (`frontend/.env.local`)

```env
NEXT_PUBLIC_API_URL=http://localhost:3001
```

#### Backend (`backend/.env`)

```env
OLLAMA_URL=http://localhost:11434  # URL d'Ollama
NODE_ENV=development
SYSTEM_PROMPT="Votre prompt systÃ¨me personnalisÃ©"
```

### Personnalisation du prompt systÃ¨me

Le prompt systÃ¨me peut Ãªtre modifiÃ© dans `backend/server.js` (ligne 281) ou via la variable d'environnement `SYSTEM_PROMPT`.

## ğŸ“– Utilisation

1. **AccÃ©der Ã  l'interface** : Ouvrez [http://localhost:3000](http://localhost:3000)

2. **Poser une question** : Tapez votre question dans la zone de texte
   - Exemple : "Quelles sont les derniÃ¨res nouvelles en IA ?"
   - Exemple : "Explique-moi l'hypothÃ¨se de Riemann"

3. **Recherche automatique** : L'application effectue automatiquement une recherche web

4. **Visualiser la rÃ©ponse** : 
   - La rÃ©ponse s'affiche en temps rÃ©el (streaming)
   - Les sources web sont affichÃ©es en bas de la rÃ©ponse
   - Cliquez sur les sources pour accÃ©der aux sites d'origine

## ğŸ“ Structure du projet

```
IA/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ server.js              # Serveur Express principal
â”‚   â”œâ”€â”€ package.json           # DÃ©pendances backend
â”‚   â”œâ”€â”€ Dockerfile             # Image Docker production
â”‚   â””â”€â”€ Dockerfile.dev         # Image Docker dÃ©veloppement
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ page.tsx          # Page principale du chat
â”‚   â”‚   â”œâ”€â”€ layout.tsx        # Layout Next.js
â”‚   â”‚   â”œâ”€â”€ globals.css       # Styles globaux
â”‚   â”‚   â””â”€â”€ test/
â”‚   â”‚       â””â”€â”€ page.tsx      # Page de test
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ ui/
â”‚   â”‚   â”‚   â”œâ”€â”€ animated-ai-chat.tsx        # Composant de chat (non utilisÃ©)
â”‚   â”‚   â”‚   â””â”€â”€ formatted-response.tsx      # Formatage des rÃ©ponses
â”‚   â”‚   â””â”€â”€ theme-provider.tsx
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â””â”€â”€ utils.ts          # Utilitaires (cn, etc.)
â”‚   â”œâ”€â”€ public/               # Ressources statiques
â”‚   â”œâ”€â”€ package.json          # DÃ©pendances frontend
â”‚   â”œâ”€â”€ next.config.ts        # Configuration Next.js
â”‚   â”œâ”€â”€ tailwind.config.js    # Configuration Tailwind
â”‚   â”œâ”€â”€ tsconfig.json         # Configuration TypeScript
â”‚   â”œâ”€â”€ Dockerfile            # Image Docker production
â”‚   â””â”€â”€ Dockerfile.dev        # Image Docker dÃ©veloppement
â”‚
â”œâ”€â”€ docker-compose.yml         # Configuration Docker production
â”œâ”€â”€ docker-compose.dev.yml     # Configuration Docker dÃ©veloppement
â””â”€â”€ README.md                  # Ce fichier
```

## ğŸ› ï¸ Technologies utilisÃ©es

### Frontend
- **Next.js 15** : Framework React avec App Router
- **React 19** : BibliothÃ¨que UI
- **TypeScript** : Typage statique
- **Tailwind CSS** : Framework CSS utilitaire
- **Framer Motion** : Animations fluides
- **Lucide React** : IcÃ´nes modernes

### Backend
- **Node.js** : Runtime JavaScript
- **Express** : Framework web
- **Axios** : Client HTTP
- **Cheerio** : Parsing HTML
- **dotenv** : Gestion des variables d'environnement

### IA & Infrastructure
- **Ollama** : ExÃ©cution locale de modÃ¨les IA
- **Mistral** : ModÃ¨le de langage
- **DuckDuckGo API** : Recherche web
- **Docker & Docker Compose** : Conteneurisation

## ğŸ”§ DÃ©veloppement

### Commandes utiles

#### Logs des services

```bash
# Tous les services
docker-compose logs -f

# Service spÃ©cifique
docker-compose logs -f backend
docker-compose logs -f frontend
docker-compose logs -f ollama
```

#### RedÃ©marrer un service

```bash
docker-compose restart backend
docker-compose restart frontend
```

#### ArrÃªter l'application

```bash
docker-compose down
```

#### Supprimer les volumes (rÃ©initialisation complÃ¨te)

```bash
docker-compose down -v
```

### Tester la recherche web

Le backend expose l'endpoint `/api/generate` qui accepte un POST avec :

```json
{
  "prompt": "Votre question"
}
```

Exemple avec curl :

```bash
curl -X POST http://localhost:3001/api/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Quelles sont les derniÃ¨res nouvelles en IA ?"}'
```

### Lister les modÃ¨les Ollama disponibles

```bash
curl http://localhost:11434/api/tags
```

Ou via l'API backend :

```bash
curl http://localhost:3001/api/models
```

## ğŸ› DÃ©pannage

### Le modÃ¨le Mistral n'est pas trouvÃ©

```bash
# VÃ©rifier que le conteneur Ollama fonctionne
docker ps | grep ollama

# TÃ©lÃ©charger le modÃ¨le
docker exec -it ia-ollama-1 ollama pull mistral:latest

# Lister les modÃ¨les installÃ©s
docker exec -it ia-ollama-1 ollama list
```

### Le frontend ne se connecte pas au backend

1. VÃ©rifier que le backend est en ligne : `curl http://localhost:3001/api/models`
2. VÃ©rifier la variable `NEXT_PUBLIC_API_URL` dans le frontend
3. VÃ©rifier les logs : `docker-compose logs backend`

### La recherche web ne fonctionne pas

1. VÃ©rifier la connexion internet
2. VÃ©rifier les logs backend pour voir les erreurs de recherche
3. DuckDuckGo peut avoir des limites de taux - attendre quelques minutes

### Performances lentes

1. Augmenter la RAM allouÃ©e Ã  Docker (minimum 8 GB recommandÃ©)
2. Ajuster les paramÃ¨tres du modÃ¨le dans `server.js` :
   - `num_ctx` : Contexte (dÃ©faut: 4096)
   - `num_thread` : Threads CPU (dÃ©faut: 4)
   - `temperature` : CrÃ©ativitÃ© (dÃ©faut: 0.7)

## ğŸ“ AmÃ©liorations futures

- [ ] Historique des conversations
- [ ] Support de plusieurs modÃ¨les IA
- [ ] Export des conversations
- [ ] Mode sombre/clair
- [ ] Support multilingue
- [ ] Cache des recherches web
- [ ] API de gestion des sources
- [ ] Tests unitaires et E2E

## ğŸ“„ Licence

Ce projet est sous licence ISC.

## ğŸ‘¨â€ğŸ’» Auteur

CrÃ©Ã© avec â¤ï¸ par Theo

---

**Note** : Ce projet utilise Ollama pour exÃ©cuter des modÃ¨les IA localement. Assurez-vous d'avoir suffisamment de ressources systÃ¨me pour faire fonctionner Mistral correctement.

